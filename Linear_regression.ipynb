{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear_regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyXzQyKijWfeFln6U1bYOn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hasan-Erbilen/Captcha_project/blob/main/Linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1uh_icUEkTm",
        "outputId": "f57a5905-9aee-4e33-9fbf-0160ff3b50ed"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "def normalise(array):\n",
        "    \"\"\"Normalise given array using standard z-score.\"\"\"\n",
        "    return (array - array.mean()) / array.std()\n",
        "\n",
        "\n",
        "def xzero_id(x):\n",
        "    \"\"\"Pad x_{1..n} with the multiplicative identity of x_0.\"\"\"\n",
        "    return np.c_[np.ones(x.size), x.reshape(-1, 1)]\n",
        "\n",
        "\n",
        "def linear_model(x, theta):\n",
        "    \"\"\"Return theta transpose x.\"\"\"\n",
        "    # Set x_0 to 1 for more convenient inner product.\n",
        "    return xzero_id(x) @ theta\n",
        "\n",
        "\n",
        "def cost_mse(x, y, theta):\n",
        "    \"\"\"Mean squared error cost function.\"\"\"\n",
        "    return ((linear_model(x, theta) - y) ** 2).mean()\n",
        "\n",
        "\n",
        "def gradient_descent(xtrain, ytrain, theta,\n",
        "                     alpha=1e-2, epsilon=1e-7, iters=256):\n",
        "    \"\"\"Return new theta calculated using gradient descent.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    xtrain : ndarray\n",
        "        Training set inputs.\n",
        "    ytrain : ndarray\n",
        "        Training set outputs.\n",
        "    theta : ndarray\n",
        "        Initial model parameters.\n",
        "    alpha : float\n",
        "        Learning rate.\n",
        "    epsilon : float\n",
        "        Termination threshold.\n",
        "    iters : int\n",
        "        Number of iterations.\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    theta : ndarray\n",
        "        New model parameters.\n",
        "    costs : ndarray\n",
        "        History of cost per iteration.\n",
        "    \"\"\"\n",
        "    x = xzero_id(xtrain)      # Training set with x_0 = 1\n",
        "    m = xtrain.size           # Size of training set\n",
        "    deriv = (-2 * alpha) / m  # Cost derivative vs learning rate\n",
        "    costs = np.zeros(iters)   # Costs history\n",
        "    theta = theta.copy()      # Avoid destructive operations\n",
        "\n",
        "    for i in range(iters):\n",
        "        delta = deriv * ((linear_model(xtrain, theta) - ytrain) @ x)\n",
        "        theta += delta\n",
        "        cost = cost_mse(xtrain, ytrain, theta)\n",
        "\n",
        "        # Update cost history.  If all deltas exceed threshold,\n",
        "        # extend cost history and quit.\n",
        "        if np.all(delta < epsilon):\n",
        "            costs[i:] = cost\n",
        "            break\n",
        "        else:\n",
        "            costs[i] = cost\n",
        "\n",
        "    return theta, costs\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Print and plot results for assignment.\"\"\"\n",
        "\n",
        "    # Read and normalise training data.\n",
        "    xtrain, ytrain = np.loadtxt('week1.csv', delimiter=',', unpack=True)\n",
        "    xtrain = normalise(xtrain)\n",
        "    ytrain = normalise(ytrain)\n",
        "\n",
        "    # Initial plot of training data.\n",
        "    plt.figure(constrained_layout=True)\n",
        "    plt.rc('font', size=16)\n",
        "    plt.xlabel('Input x (standardised)')\n",
        "    plt.ylabel('Output y (standardised)')\n",
        "    plt.scatter(xtrain, ytrain, s=16, c='dimgray', marker='+',\n",
        "                label='Training data')\n",
        "    plt.savefig('0-train.pdf')\n",
        "\n",
        "    # Pick baseline parameters suitable for standardised data.\n",
        "    base_theta = np.zeros(2)\n",
        "    base_model = linear_model(xtrain, base_theta)\n",
        "\n",
        "    # Train model.\n",
        "    theta, costs = gradient_descent(xtrain, ytrain, base_theta)\n",
        "    ypred = linear_model(xtrain, theta)\n",
        "    print('Parameters:', theta)\n",
        "    print('Cost:', {'baseline': cost_mse(xtrain, ytrain, base_theta),\n",
        "                    'trained': costs[-1]})\n",
        "\n",
        "    # Plot against sklearn and baseline models.\n",
        "    plt.plot(xtrain, ypred, linewidth=3.0, label='Predictions (vanilla)')\n",
        "\n",
        "    xtrans = xtrain.reshape(-1, 1)\n",
        "    ypred = LinearRegression().fit(xtrans, ytrain).predict(xtrans)\n",
        "\n",
        "    plt.plot(xtrain, ypred, ':', linewidth=5.0, label='Predictions (sklearn)')\n",
        "    plt.plot(xtrain, base_model, '--', linewidth=2.0, label='Baseline')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.savefig('1-predict.pdf')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot learning rates against cost.\n",
        "    plt.figure(constrained_layout=True)\n",
        "    plt.xlabel('Iteration number')\n",
        "    plt.ylabel('J(θ)')\n",
        "\n",
        "    for alpha in (1e-3, 1e-2, 1e-1, 9e-1):\n",
        "        costs = gradient_descent(xtrain, ytrain, base_theta, alpha=alpha)[1]\n",
        "        plt.plot(costs, linewidth=2.0, label=f'α = {alpha}')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.savefig('2-costs.pdf')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters: [-1.49182064e-17 -1.89459783e-02]\n",
            "Cost: {'baseline': 1.0000000000000002, 'trained': 0.964463940888146}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKysTGb7RgYS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}